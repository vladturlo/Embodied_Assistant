# Ollama Model Configuration for qwen3-vl:2b
# Used by the multimodal agent for vision and text processing
#
# For local Ollama (default):
#   - Model: qwen3-vl:2b (fits on 4GB VRAM)
#   - Host: localhost:11434
#
# For remote Ollama (SSH tunnel):
#   - Model: qwen3-vl:238b
#   - Host: localhost:11435

provider: ollama
model: qwen3-vl:235b
host: http://localhost:11435

# Model capabilities
capabilities:
  vision: true
  function_calling: true
  json_output: false
  structured_output: false

# Generation options
options:
  temperature: 0.7
  num_ctx: 32768  # 32K context window (suitable for 2B model)
  # top_p: 0.9
  # top_k: 40
  # repeat_penalty: 1.1

# Model info for AutoGen
model_info:
  vision: true
  function_calling: true
  json_output: false
  family: unknown
  structured_output: false
