# Model Configuration
# Supports llama.cpp server (OpenAI-compatible) or Ollama
#
# For llama.cpp server, start with:
#   llama-server -m model.gguf --mmproj mmproj.gguf -c 262144 -np 1 --host 0.0.0.0 --port 11434 -sps 0.0
#
# Available Qwen3-VL GGUF models (https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct-GGUF):
#   - Qwen3VL-8B-Instruct-Q4_K_M.gguf   (fits on 16GB VRAM)
#   - Qwen3VL-32B-Instruct-Q4_K_M.gguf  (fits on 48GB VRAM)

provider: llamacpp

# Model name (as loaded in llama-server)
model: ministral-3:8b

# llama.cpp OpenAI-compatible endpoint
base_url: http://localhost:11435/v1
api_key: not-needed

# Model capabilities
capabilities:
  vision: true
  function_calling: true
  json_output: false
  structured_output: false

# Generation options
options:
  temperature: 0.15  # Low temperature as recommended by Mistral
  num_ctx: 262144  # 256K full context

# llama.cpp specific options for KV cache persistence
# These are passed via extra_body in the OpenAI request
llamacpp:
  id_slot: 0           # Dedicated slot for embodied mode (KV cache affinity)
  cache_prompt: true   # Enable KV cache reuse for matching prefixes

# Model info for AutoGen
model_info:
  vision: true
  function_calling: true
  json_output: false
  family: mistral
  structured_output: false

# Video processing settings
video:
  frames_per_second: 5.0  # Frames to extract per second of video
  max_frames: 50          # Maximum frames to prevent overload on long videos

# Image capture settings for embodied control
# Lower resolution = faster model inference + fewer visual tokens
image:
  max_width: 480      # Maximum image width (reduced from 640 for speed)
  max_height: 360     # Maximum image height (reduced from 480 for speed)
  jpeg_quality: 65    # JPEG compression quality (0-100)
