# Ollama Model Configuration for qwen3-vl:2b
# Used by the multimodal agent for vision and text processing
#
# For local Ollama (default):
#   - Model: qwen3-vl:2b (fits on 4GB VRAM)
#   - Host: localhost:11434
#
# For remote Ollama (SSH tunnel):
#   - Model: qwen3-vl:238b
#   - Host: localhost:11435

provider: ollama
model: qwen3-vl:235b
host: http://localhost:11435

# Model capabilities
capabilities:
  vision: true
  function_calling: true
  json_output: false
  structured_output: false

# Generation options
options:
  temperature: 0.7
  num_ctx: 32768  # 32K context window (suitable for 2B model)
  # top_p: 0.9
  # top_k: 40
  # repeat_penalty: 1.1

# Model info for AutoGen
model_info:
  vision: true
  function_calling: true
  json_output: false
  family: unknown
  structured_output: false

# Video processing settings
video:
  frames_per_second: 5.0  # Frames to extract per second of video
  max_frames: 50          # Maximum frames to prevent overload on long videos

# Live Vision mode settings
# Enables continuous camera streaming so the model can see without tool calls
live_vision:
  enabled: false              # Default state when app starts
  capture_fps: 2.0            # Frames per second to capture (1-5 recommended)
  buffer_seconds: 5.0         # Keep last N seconds of frames
  max_frames_per_message: 10  # Max frames to inject per user message
  preview_fps: 2.0            # UI preview refresh rate
  inactivity_timeout: 300     # Auto-stop after N seconds of no activity (5 min)
  add_timestamp_overlay: true # Add timestamp text to frames
