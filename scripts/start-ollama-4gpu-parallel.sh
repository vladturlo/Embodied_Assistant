#!/usr/bin/env bash
# Start N independent Ollama instances (one per GPU) behind a LiteLLM load balancer.
#
# Usage:
#   ./scripts/start-ollama-4gpu-parallel.sh <model_name> [--gpus N] [--parallel N]
#
# Examples:
#   ./scripts/start-ollama-4gpu-parallel.sh ministral-3:8b
#   ./scripts/start-ollama-4gpu-parallel.sh ministral-3:8b --gpus 4 --parallel 2
#
# Prerequisites:
#   - Ollama installed and on PATH
#   - LiteLLM installed (venv at ~/em12/vturlo_daint/venvs/litellm-env/)
#   - NVIDIA GPUs visible via nvidia-smi
#
# Architecture:
#   Client → LiteLLM (:11400) → Ollama instances (:11434, :11435, ...)
#   Each Ollama instance is pinned to one GPU via CUDA_VISIBLE_DEVICES.

set -euo pipefail

# ---------- Parse arguments ----------
MODEL=""
NUM_GPUS=""
NUM_PARALLEL=2  # slots per GPU
LITELLM_PORT=11400
OLLAMA_BASE_PORT=11434

while [[ $# -gt 0 ]]; do
    case "$1" in
        --gpus)   NUM_GPUS="$2"; shift 2 ;;
        --parallel) NUM_PARALLEL="$2"; shift 2 ;;
        --port)   LITELLM_PORT="$2"; shift 2 ;;
        -*)       echo "Unknown flag: $1"; exit 1 ;;
        *)        MODEL="$1"; shift ;;
    esac
done

if [[ -z "$MODEL" ]]; then
    echo "Usage: $0 <model_name> [--gpus N] [--parallel N]"
    echo "Example: $0 ministral-3:8b"
    exit 1
fi

# Auto-detect GPU count if not specified
if [[ -z "$NUM_GPUS" ]]; then
    NUM_GPUS=$(nvidia-smi -L 2>/dev/null | wc -l)
    if [[ "$NUM_GPUS" -eq 0 ]]; then
        echo "ERROR: No GPUs detected. Use --gpus N to override."
        exit 1
    fi
fi

TOTAL_SLOTS=$((NUM_GPUS * NUM_PARALLEL))
echo "============================================================"
echo "Multi-GPU Ollama + LiteLLM Startup"
echo "============================================================"
echo "Model:          $MODEL"
echo "GPUs:           $NUM_GPUS"
echo "Parallel/GPU:   $NUM_PARALLEL"
echo "Total slots:    $TOTAL_SLOTS"
echo "Ollama ports:   $OLLAMA_BASE_PORT - $((OLLAMA_BASE_PORT + NUM_GPUS - 1))"
echo "LiteLLM port:   $LITELLM_PORT"
echo "============================================================"
echo ""

# ---------- Kill existing processes ----------
echo "Stopping existing Ollama/LiteLLM processes..."
pkill -f "ollama serve" 2>/dev/null || true
pkill -f "litellm" 2>/dev/null || true
sleep 2

# ---------- Prepare directories ----------
WORK_DIR="${HOME}/.ollama-multi-gpu"
mkdir -p "$WORK_DIR"

# ---------- Start Ollama instances ----------
for i in $(seq 0 $((NUM_GPUS - 1))); do
    PORT=$((OLLAMA_BASE_PORT + i))
    OLLAMA_DIR="${WORK_DIR}/gpu${i}"
    mkdir -p "$OLLAMA_DIR"

    echo "Starting Ollama on GPU $i (port $PORT)..."
    CUDA_VISIBLE_DEVICES=$i \
    OLLAMA_HOST="127.0.0.1:${PORT}" \
    OLLAMA_MODELS="${HOME}/.ollama/models" \
    OLLAMA_NUM_PARALLEL=$NUM_PARALLEL \
    OLLAMA_KEEP_ALIVE=-1 \
    ollama serve > "${WORK_DIR}/ollama-gpu${i}.log" 2>&1 &
    echo "  PID: $!"
done

# ---------- Wait for all instances ----------
echo ""
echo "Waiting for Ollama instances to become ready..."
for i in $(seq 0 $((NUM_GPUS - 1))); do
    PORT=$((OLLAMA_BASE_PORT + i))
    for attempt in $(seq 1 30); do
        if curl -sf "http://127.0.0.1:${PORT}/api/tags" > /dev/null 2>&1; then
            echo "  GPU $i (:$PORT) ready"
            break
        fi
        if [[ $attempt -eq 30 ]]; then
            echo "  ERROR: GPU $i (:$PORT) failed to start. Check ${WORK_DIR}/ollama-gpu${i}.log"
            exit 1
        fi
        sleep 1
    done
done

# ---------- Preload model on all GPUs ----------
echo ""
echo "Preloading model '$MODEL' on all $NUM_GPUS GPUs..."
for i in $(seq 0 $((NUM_GPUS - 1))); do
    PORT=$((OLLAMA_BASE_PORT + i))
    # Trigger model load by sending a minimal request
    curl -sf "http://127.0.0.1:${PORT}/api/chat" \
        -d "{\"model\": \"${MODEL}\", \"messages\": [{\"role\": \"user\", \"content\": \"hi\"}], \"stream\": false}" \
        > /dev/null 2>&1 &
done
wait
echo "  Model loaded on all GPUs."

# ---------- Generate LiteLLM config ----------
CONFIG_PATH="${WORK_DIR}/litellm-config.yaml"
echo ""
echo "Generating LiteLLM config at ${CONFIG_PATH}..."

# Extract model name without tag for the LiteLLM model_name
MODEL_SHORT="${MODEL%%:*}"

cat > "$CONFIG_PATH" << YAML
# Auto-generated by start-ollama-4gpu-parallel.sh
# Model: ${MODEL} across ${NUM_GPUS} GPUs (${TOTAL_SLOTS} total slots)

model_list:
YAML

for i in $(seq 0 $((NUM_GPUS - 1))); do
    PORT=$((OLLAMA_BASE_PORT + i))
    cat >> "$CONFIG_PATH" << YAML
  - model_name: ${MODEL_SHORT}
    litellm_params:
      model: openai/${MODEL}
      api_base: http://127.0.0.1:${PORT}/v1
      api_key: not-needed
    model_info:
      supports_vision: true
YAML
done

cat >> "$CONFIG_PATH" << YAML

router_settings:
  routing_strategy: least-busy
YAML

echo "  Config written."

# ---------- Start LiteLLM ----------
echo ""
echo "Starting LiteLLM proxy on port ${LITELLM_PORT}..."

# Try to find LiteLLM
LITELLM_VENV="${HOME}/em12/vturlo_daint/venvs/litellm-env"
if [[ -f "${LITELLM_VENV}/bin/litellm" ]]; then
    LITELLM_CMD="${LITELLM_VENV}/bin/litellm"
elif command -v litellm &>/dev/null; then
    LITELLM_CMD="litellm"
else
    echo "ERROR: litellm not found. Install it or update LITELLM_VENV path."
    exit 1
fi

$LITELLM_CMD --config "$CONFIG_PATH" --port "$LITELLM_PORT" \
    > "${WORK_DIR}/litellm.log" 2>&1 &
LITELLM_PID=$!
echo "  PID: $LITELLM_PID"

# Wait for LiteLLM
for attempt in $(seq 1 30); do
    if curl -sf "http://127.0.0.1:${LITELLM_PORT}/health" > /dev/null 2>&1; then
        echo "  LiteLLM ready!"
        break
    fi
    if [[ $attempt -eq 30 ]]; then
        echo "  ERROR: LiteLLM failed to start. Check ${WORK_DIR}/litellm.log"
        exit 1
    fi
    sleep 1
done

# ---------- Summary ----------
echo ""
echo "============================================================"
echo "ALL SERVICES RUNNING"
echo "============================================================"
echo ""
echo "Connect at: http://127.0.0.1:${LITELLM_PORT}"
echo "Model name: ${MODEL_SHORT}"
echo ""
echo "SSH tunnel (from client):"
echo "  ssh -A -N -L ${LITELLM_PORT}:localhost:${LITELLM_PORT} \$(hostname)"
echo ""
echo "Test:"
echo "  curl http://127.0.0.1:${LITELLM_PORT}/v1/chat/completions \\"
echo "    -H 'Content-Type: application/json' \\"
echo "    -d '{\"model\":\"${MODEL_SHORT}\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello\"}],\"max_tokens\":20}'"
echo ""
echo "Logs:"
for i in $(seq 0 $((NUM_GPUS - 1))); do
    echo "  GPU $i: ${WORK_DIR}/ollama-gpu${i}.log"
done
echo "  LiteLLM: ${WORK_DIR}/litellm.log"
echo ""
echo "Stop all:"
echo "  pkill -f 'ollama serve' && pkill -f litellm"
